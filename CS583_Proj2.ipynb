{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2012 President Twitter Sentiment Classification\n",
    "\n",
    "### Fangda Fan, Xiaohan Liu\n",
    "\n",
    "### March 2016\n",
    "\n",
    "## Data\n",
    "\n",
    "Tweets related with 2012 US president candidates Obama and Romney (each about 7200 tweets) during October 12-16, 2012\n",
    "\n",
    "Sentiments labelled with -1 (negative), 0 (neutral), 1 (positive) and 2 (mixed).\n",
    "\n",
    "Need to classify tweets with -1, 0, and 1 (2 is omitted).\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Platform: [Anaconda 4.3](https://www.continuum.io/downloads) (Python 3.6)\n",
    "\n",
    "- Package for neural network: [Keras](https://github.com/fchollet/keras) with [Tensorflow](https://www.tensorflow.org/) or Theano backend\n",
    "```sh\n",
    "pip install keras\n",
    "```\n",
    "- Package for dataframe-based machine learning: [DFlearn](https://github.com/founderfan/DFlearn)\n",
    "```sh\n",
    "pip install dflearn\n",
    "```\n",
    "- Packages for advanced gradient boosting models: [LightGBM](https://github.com/Microsoft/LightGBM) and [XGBoost](https://github.com/dmlc/xgboost)\n",
    "- NLTK package data download in python: \n",
    "```python \n",
    "nltk.download()\n",
    "```\n",
    "- [GloVe](http://nlp.stanford.edu/projects/glove/) 27B twitter dictionary\n",
    "\n",
    "## Data Mining Methods\n",
    "\n",
    "### 1. Bag-of-Word Analysis\n",
    "\n",
    "Includes the following features:\n",
    "1. TF-IDF vectorization of sentences\n",
    "2. [Liu](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon)'s opinion lexicon counting\n",
    "3. [Vader](http://www.nltk.org/_modules/nltk/sentiment/vader.html) sentiment analyzer\n",
    "4. Date-time variables\n",
    "\n",
    "Models:\n",
    "\n",
    "- Bernoulli Naive Bayes\n",
    "- Random Forest\n",
    "- AdaBoost\n",
    "- Gradient Boosting Tree\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "- SVM\n",
    "\n",
    "### 2. Sequential Word Analysis\n",
    "\n",
    "Includes the following features:\n",
    "\n",
    "1. Word vectorization from [GloVe](http://nlp.stanford.edu/projects/glove/) 27B twitter dictionary\n",
    "2. [Part-of-speech tagging](http://www.nltk.org/book/ch05.html)\n",
    "3. [SentiWordNet](http://sentiwordnet.isti.cnr.it/)\n",
    "\n",
    "Models:\n",
    "\n",
    "- Deep Neural Network: CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": "-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/founderfan/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import sklearn.preprocessing as prep\n",
    "import sklearn.feature_extraction.text as txt\n",
    "import sklearn.naive_bayes as nbayes\n",
    "import sklearn.svm as svm\n",
    "import sklearn.ensemble as ensm\n",
    "import sklearn.metrics as met\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import keras as kr\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "import nltk.sentiment as sent\n",
    "import dflearn.MLtools as mt\n",
    "import dflearn.NLtools as nt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Clean Data\n",
    "\n",
    "- Clean date into days (as all data comes from 2012/10/12 to 2012/10/16\n",
    "- Clean time into hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_time(x):\n",
    "    if(len(x)<3):\n",
    "        return(np.nan)\n",
    "    else:\n",
    "        return(x[0]+x[1]/60+x[2]/3600)\n",
    "    \n",
    "word_normalize = nt.word_Normalizer()\n",
    "word_tokenize = nt.word_Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7200, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da = pd.read_excel(\"tweet.xlsx\", sheetname=1).apply(lambda x: x.astype(\"str\").str.strip(\"\\t \"))\n",
    "da[\"date\"] = da[\"date\"].apply(mt.strnum, f_reduce = lambda x: np.sort(x)[-2])\n",
    "da[\"time\"] = da[\"time\"].str.split(\"-\", expand = True)[0].apply(mt.strnum, f_reduce = clean_time).where(~da[\"time\"].str.contains(\"M\"), lambda x: x%12) + 12*da[\"time\"].str.contains(\"PM\")\n",
    "da[\"Class\"] = pd.to_numeric(da[\"Class\"], errors = \"coerce\")\n",
    "da[\"Anootated tweet\"] = da[\"Anootated tweet\"].str.replace(\"</e>\", \"<e>\").str.replace(\"<e>\", \"<em>\").str.replace(\"</a>\", \"<a>\").str.lower()\n",
    "da.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    insidious ! <em> mitt romney <em> ' s bain hel...\n",
       "1    senior <em> romney <em> advisor claims <em> ob...\n",
       "2    . @wardbrenda @shortwave8669 @allanbourdius yo...\n",
       "3    <em> mitt romney <em> still doesn't <a> believ...\n",
       "4    <em> romney <em> ' s <a> tax plan <a> deserves...\n",
       "Name: Anootated tweet, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = da[\"Anootated tweet\"].apply(lambda x: \" \".join(word_tokenize.transform(x)))\n",
    "S.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bag-of-Word Analysis\n",
    "\n",
    "- Use TF-IDF with lemmatization for words in the text\n",
    "- Use polarity score of Vadar sentiment analysis\n",
    "- Load Sentiwordnet dictionary for positive, negative and objective attributes of words (used in sequential word model)\n",
    "- Get word vector for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>\"</th>\n",
       "      <th>#</th>\n",
       "      <th>#012</th>\n",
       "      <th>#17electionistas</th>\n",
       "      <th>#18</th>\n",
       "      <th>#1u</th>\n",
       "      <th>#2012</th>\n",
       "      <th>#2012debate</th>\n",
       "      <th>...</th>\n",
       "      <th>üí∫</th>\n",
       "      <th>üóΩ</th>\n",
       "      <th>üòÇ</th>\n",
       "      <th>üòâ</th>\n",
       "      <th>üòí</th>\n",
       "      <th>üòì</th>\n",
       "      <th>üòù</th>\n",
       "      <th>üò•</th>\n",
       "      <th>üò≠</th>\n",
       "      <th>üò±</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.112244</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 9968 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               !    \"    #  #012  #17electionistas  #18  #1u  #2012  \\\n",
       "0  0.0  0.112244  0.0  0.0   0.0               0.0  0.0  0.0    0.0   \n",
       "1  0.0  0.000000  0.0  0.0   0.0               0.0  0.0  0.0    0.0   \n",
       "2  0.0  0.000000  0.0  0.0   0.0               0.0  0.0  0.0    0.0   \n",
       "3  0.0  0.000000  0.0  0.0   0.0               0.0  0.0  0.0    0.0   \n",
       "4  0.0  0.000000  0.0  0.0   0.0               0.0  0.0  0.0    0.0   \n",
       "\n",
       "   #2012debate ...     üí∫    üóΩ    üòÇ    üòâ    üòí    üòì    üòù    üò•    üò≠    üò±  \n",
       "0          0.0 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1          0.0 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2          0.0 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3          0.0 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4          0.0 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 9968 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = txt.TfidfVectorizer(analyzer=lambda x: word_tokenize.transform(x, word_normalize.transform))\n",
    "X_word = pd.DataFrame(vectorizer.fit_transform(S).toarray(), columns = vectorizer.get_feature_names(), dtype = \"float16\")\n",
    "X_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((117659, 5), (86571, 3), (9968, 3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daa_sent = pd.DataFrame([(i.synset.name()[:-5], i.synset.name()[-4], i.pos_score(), i.neg_score(), i.obj_score()) for i in nltk.corpus.sentiwordnet.all_senti_synsets()], columns=[\"word\", \"attr\", \"pos_swd\", \"neg_swd\", \"obj_swd\"], dtype=\"float16\")\n",
    "daa_sent_c = daa_sent.groupby(\"word\").mean()\n",
    "daa_sent_word = daa_sent_c.reindex(X_word.columns).fillna({\"pos_swd\": 0, \"neg_swd\": 0, \"obj_swd\": 1})\n",
    "daa_sent.shape, daa_sent_c.shape, daa_sent_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_sent = pd.concat([X_word.loc[:, X_word.columns.intersection(getattr(nltk.corpus.opinion_lexicon, i)())].sum(axis=1) for i in [\"positive\", \"negative\"]], axis=1, keys = [\"pos_sum\", 'neg_sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_sum</th>\n",
       "      <th>neg_sum</th>\n",
       "      <th>compound</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355469</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295654</td>\n",
       "      <td>-0.4019</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.185303</td>\n",
       "      <td>0.356689</td>\n",
       "      <td>-0.2023</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184570</td>\n",
       "      <td>-0.5267</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pos_sum   neg_sum  compound    neg    neu   pos\n",
       "0  0.000000  0.355469    0.0000  0.000  1.000  0.00\n",
       "1  0.000000  0.295654   -0.4019  0.119  0.881  0.00\n",
       "2  0.185303  0.356689   -0.2023  0.185  0.674  0.14\n",
       "3  0.000000  0.000000    0.0000  0.000  1.000  0.00\n",
       "4  0.000000  0.184570   -0.5267  0.116  0.884  0.00"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sent = X_sent.join(pd.DataFrame.from_records(S.apply(sent.vader.SentimentIntensityAnalyzer().polarity_scores)))\n",
    "X_sent.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7200, 9853)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([da[[\"date\", \"time\"]], X_sent, X_word.loc[:,~X_word.columns.isin(nltk.corpus.stopwords.words(\"english\"))].rename(columns = lambda x: \"w_{}\".format(x))], axis = 1, copy = False)\n",
    "X[\"word\"] = (X_word > 0).sum(axis = 1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5648, 754), (5648, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir = da[\"Class\"].loc[da[\"Class\"].isin([-1,0,1])].index\n",
    "mdset = mt.CVdata(df = X.join(da[\"Class\"]), ic_x = X.columns, ic_y = [\"Class\"], ir = ir, k = 10, sp = 0.002, \n",
    "                  f_norm = lambda x: x.fillna(x.mean()).astype(\"float16\").rename(columns=lambda x: x.replace(\"<\", \"_a(\").replace(\"[\", \"_s(\").replace(\"]\", \"_s)\")))\n",
    "mdset[\"X\"].shape, mdset[\"Y\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namemd</th>\n",
       "      <th>f_model</th>\n",
       "      <th>par_model</th>\n",
       "      <th>f_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adb</th>\n",
       "      <td>ADB</td>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBo...</td>\n",
       "      <td>{'n_estimators': 500, 'learning_rate': 0.1}</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bnb</th>\n",
       "      <td>BNB</td>\n",
       "      <td>&lt;class 'sklearn.naive_bayes.BernoulliNB'&gt;</td>\n",
       "      <td>{}</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb</th>\n",
       "      <td>GB</td>\n",
       "      <td>&lt;class 'sklearn.ensemble.gradient_boosting.Gra...</td>\n",
       "      <td>{'n_estimators': 100, 'learning_rate': 0.1, 'm...</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgb</th>\n",
       "      <td>LGB</td>\n",
       "      <td>&lt;class 'lightgbm.sklearn.LGBMClassifier'&gt;</td>\n",
       "      <td>{'n_estimators': 100, 'colsample_bytree': 0.3,...</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbdart</th>\n",
       "      <td>LGB DART</td>\n",
       "      <td>&lt;class 'lightgbm.sklearn.LGBMClassifier'&gt;</td>\n",
       "      <td>{'boosting_type': 'dart', 'n_estimators': 100,...</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>RF</td>\n",
       "      <td>&lt;class 'sklearn.ensemble.forest.RandomForestCl...</td>\n",
       "      <td>{'n_estimators': 500}</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>SVM</td>\n",
       "      <td>&lt;class 'sklearn.svm.classes.LinearSVC'&gt;</td>\n",
       "      <td>{'dual': False}</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>XGB</td>\n",
       "      <td>&lt;class 'xgboost.sklearn.XGBClassifier'&gt;</td>\n",
       "      <td>{'n_estimators': 100, 'colsample_bylevel': 0.2...</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           namemd                                            f_model  \\\n",
       "adb           ADB  <class 'sklearn.ensemble.weight_boosting.AdaBo...   \n",
       "bnb           BNB          <class 'sklearn.naive_bayes.BernoulliNB'>   \n",
       "gb             GB  <class 'sklearn.ensemble.gradient_boosting.Gra...   \n",
       "lgb           LGB          <class 'lightgbm.sklearn.LGBMClassifier'>   \n",
       "lgbdart  LGB DART          <class 'lightgbm.sklearn.LGBMClassifier'>   \n",
       "rf             RF  <class 'sklearn.ensemble.forest.RandomForestCl...   \n",
       "svm           SVM            <class 'sklearn.svm.classes.LinearSVC'>   \n",
       "xgb           XGB            <class 'xgboost.sklearn.XGBClassifier'>   \n",
       "\n",
       "                                                 par_model  \\\n",
       "adb            {'n_estimators': 500, 'learning_rate': 0.1}   \n",
       "bnb                                                     {}   \n",
       "gb       {'n_estimators': 100, 'learning_rate': 0.1, 'm...   \n",
       "lgb      {'n_estimators': 100, 'colsample_bytree': 0.3,...   \n",
       "lgbdart  {'boosting_type': 'dart', 'n_estimators': 100,...   \n",
       "rf                                   {'n_estimators': 500}   \n",
       "svm                                        {'dual': False}   \n",
       "xgb      {'n_estimators': 100, 'colsample_bylevel': 0.2...   \n",
       "\n",
       "                                             f_loss  \n",
       "adb      <function zero_one_loss at 0x7f1d2ea35ae8>  \n",
       "bnb      <function zero_one_loss at 0x7f1d2ea35ae8>  \n",
       "gb       <function zero_one_loss at 0x7f1d2ea35ae8>  \n",
       "lgb      <function zero_one_loss at 0x7f1d2ea35ae8>  \n",
       "lgbdart  <function zero_one_loss at 0x7f1d2ea35ae8>  \n",
       "rf       <function zero_one_loss at 0x7f1d2ea35ae8>  \n",
       "svm      <function zero_one_loss at 0x7f1d2ea35ae8>  \n",
       "xgb      <function zero_one_loss at 0x7f1d2ea35ae8>  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdpar_df = pd.DataFrame.from_dict({\n",
    "    \"bnb\": {\"namemd\": \"BNB\", \"f_model\": nbayes.BernoulliNB, \"par_model\": {}},\n",
    "    \"rf\": {\"namemd\": \"RF\", \"f_model\": ensm.RandomForestClassifier, \"par_model\": {\"n_estimators\": 500}},\n",
    "    \"adb\": {\"namemd\": \"ADB\", \"f_model\": ensm.AdaBoostClassifier, \"par_model\": {\"n_estimators\": 500, \"learning_rate\": 0.1}},\n",
    "    \"gb\": {\"namemd\": \"GB\", \"f_model\": ensm.GradientBoostingClassifier, \"par_model\": {\"n_estimators\": 100, \"learning_rate\": 0.1, \"max_depth\": 3, \"max_features\": 0.2}},\n",
    "    \"xgb\": {\"namemd\": \"XGB\", \"f_model\": xgb.XGBClassifier, \"par_model\": {\"n_estimators\": 100, \"colsample_bylevel\": 0.2, \"max_depth\": 5, 'learning_rate': 0.1, 'eval_metric': 'merror'}},\n",
    "    \"lgb\": {\"namemd\": \"LGB\", \"f_model\": lgb.LGBMClassifier, \"par_model\": {\"n_estimators\": 100, \"colsample_bytree\": 0.3, \"num_leaves\": 32, \"learning_rate\": 0.1, \"subsample_for_bin\": 10, 'eval_metric': 'multiclass'}},\n",
    "    \"lgbdart\":  {\"namemd\": \"LGB DART\", \"f_model\": lgb.LGBMClassifier, \"par_model\": {\"boosting_type\": \"dart\", \"n_estimators\": 100, \"colsample_bytree\": 0.3, \"num_leaves\": 32, \"learning_rate\": 0.1, \"subsample_for_bin\": 10, 'eval_metric': 'multiclass'}},\n",
    "    \"svm\": {\"namemd\": \"SVM\", \"f_model\": svm.LinearSVC, \"par_model\": {\"dual\": False}}}, orient = \"index\")\n",
    "mdpar_df[\"f_loss\"] = met.zero_one_loss\n",
    "mdpar_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36460176991150439"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xytv = mt.CVset(**mdset)\n",
    "model = mt.MDinit(**mdpar_df.loc[\"rf\"].to_dict())\n",
    "mt.MDfit(model, **xytv)\n",
    "mt.Loss(xytv[\"yv\"].values, model.predict(xytv['xv']), met.zero_one_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xt</th>\n",
       "      <th>xv</th>\n",
       "      <th>yt</th>\n",
       "      <th>yv</th>\n",
       "      <th>model</th>\n",
       "      <th>namemd</th>\n",
       "      <th>f_loss</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>Class\n",
       "2      -1.0\n",
       "3      -1.0\n",
       "4      -1....</td>\n",
       "      <td>Class\n",
       "0      -1.0\n",
       "5       1.0\n",
       "7      -1....</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>ADB</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "      <td>0.375221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>Class\n",
       "2      -1.0\n",
       "3      -1.0\n",
       "4      -1....</td>\n",
       "      <td>Class\n",
       "0      -1.0\n",
       "5       1.0\n",
       "7      -1....</td>\n",
       "      <td>BernoulliNB(alpha=1.0, binarize=0.0, class_pri...</td>\n",
       "      <td>BNB</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "      <td>0.403540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>Class\n",
       "2      -1.0\n",
       "3      -1.0\n",
       "4      -1....</td>\n",
       "      <td>Class\n",
       "0      -1.0\n",
       "5       1.0\n",
       "7      -1....</td>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>GB</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "      <td>0.373451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>Class\n",
       "2      -1.0\n",
       "3      -1.0\n",
       "4      -1....</td>\n",
       "      <td>Class\n",
       "0      -1.0\n",
       "5       1.0\n",
       "7      -1....</td>\n",
       "      <td>LGBMClassifier(boosting_type='gbdt', colsample...</td>\n",
       "      <td>LGB</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "      <td>0.364602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>Class\n",
       "2      -1.0\n",
       "3      -1.0\n",
       "4      -1....</td>\n",
       "      <td>Class\n",
       "0      -1.0\n",
       "5       1.0\n",
       "7      -1....</td>\n",
       "      <td>LGBMClassifier(boosting_type='dart', colsample...</td>\n",
       "      <td>LGB DART</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "      <td>0.373451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>Class\n",
       "2      -1.0\n",
       "3      -1.0\n",
       "4      -1....</td>\n",
       "      <td>Class\n",
       "0      -1.0\n",
       "5       1.0\n",
       "7      -1....</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>RF</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "      <td>0.364602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>Class\n",
       "2      -1.0\n",
       "3      -1.0\n",
       "4      -1....</td>\n",
       "      <td>Class\n",
       "0      -1.0\n",
       "5       1.0\n",
       "7      -1....</td>\n",
       "      <td>LinearSVC(C=1.0, class_weight=None, dual=False...</td>\n",
       "      <td>SVM</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>Class\n",
       "2      -1.0\n",
       "3      -1.0\n",
       "4      -1....</td>\n",
       "      <td>Class\n",
       "0      -1.0\n",
       "5       1.0\n",
       "7      -1....</td>\n",
       "      <td>XGBClassifier(base_score=0.5, colsample_byleve...</td>\n",
       "      <td>XGB</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "      <td>0.385841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  xt  \\\n",
       "0        date       time   pos_sum   neg_sum  com...   \n",
       "1        date       time   pos_sum   neg_sum  com...   \n",
       "2        date       time   pos_sum   neg_sum  com...   \n",
       "3        date       time   pos_sum   neg_sum  com...   \n",
       "4        date       time   pos_sum   neg_sum  com...   \n",
       "5        date       time   pos_sum   neg_sum  com...   \n",
       "6        date       time   pos_sum   neg_sum  com...   \n",
       "7        date       time   pos_sum   neg_sum  com...   \n",
       "\n",
       "                                                  xv  \\\n",
       "0        date       time   pos_sum   neg_sum  com...   \n",
       "1        date       time   pos_sum   neg_sum  com...   \n",
       "2        date       time   pos_sum   neg_sum  com...   \n",
       "3        date       time   pos_sum   neg_sum  com...   \n",
       "4        date       time   pos_sum   neg_sum  com...   \n",
       "5        date       time   pos_sum   neg_sum  com...   \n",
       "6        date       time   pos_sum   neg_sum  com...   \n",
       "7        date       time   pos_sum   neg_sum  com...   \n",
       "\n",
       "                                                  yt  \\\n",
       "0        Class\n",
       "2      -1.0\n",
       "3      -1.0\n",
       "4      -1....   \n",
       "1        Class\n",
       "2      -1.0\n",
       "3      -1.0\n",
       "4      -1....   \n",
       "2        Class\n",
       "2      -1.0\n",
       "3      -1.0\n",
       "4      -1....   \n",
       "3        Class\n",
       "2      -1.0\n",
       "3      -1.0\n",
       "4      -1....   \n",
       "4        Class\n",
       "2      -1.0\n",
       "3      -1.0\n",
       "4      -1....   \n",
       "5        Class\n",
       "2      -1.0\n",
       "3      -1.0\n",
       "4      -1....   \n",
       "6        Class\n",
       "2      -1.0\n",
       "3      -1.0\n",
       "4      -1....   \n",
       "7        Class\n",
       "2      -1.0\n",
       "3      -1.0\n",
       "4      -1....   \n",
       "\n",
       "                                                  yv  \\\n",
       "0        Class\n",
       "0      -1.0\n",
       "5       1.0\n",
       "7      -1....   \n",
       "1        Class\n",
       "0      -1.0\n",
       "5       1.0\n",
       "7      -1....   \n",
       "2        Class\n",
       "0      -1.0\n",
       "5       1.0\n",
       "7      -1....   \n",
       "3        Class\n",
       "0      -1.0\n",
       "5       1.0\n",
       "7      -1....   \n",
       "4        Class\n",
       "0      -1.0\n",
       "5       1.0\n",
       "7      -1....   \n",
       "5        Class\n",
       "0      -1.0\n",
       "5       1.0\n",
       "7      -1....   \n",
       "6        Class\n",
       "0      -1.0\n",
       "5       1.0\n",
       "7      -1....   \n",
       "7        Class\n",
       "0      -1.0\n",
       "5       1.0\n",
       "7      -1....   \n",
       "\n",
       "                                               model    namemd  \\\n",
       "0  (DecisionTreeClassifier(class_weight=None, cri...       ADB   \n",
       "1  BernoulliNB(alpha=1.0, binarize=0.0, class_pri...       BNB   \n",
       "2  ([DecisionTreeRegressor(criterion='friedman_ms...        GB   \n",
       "3  LGBMClassifier(boosting_type='gbdt', colsample...       LGB   \n",
       "4  LGBMClassifier(boosting_type='dart', colsample...  LGB DART   \n",
       "5  (DecisionTreeClassifier(class_weight=None, cri...        RF   \n",
       "6  LinearSVC(C=1.0, class_weight=None, dual=False...       SVM   \n",
       "7  XGBClassifier(base_score=0.5, colsample_byleve...       XGB   \n",
       "\n",
       "                                       f_loss      loss  \n",
       "0  <function zero_one_loss at 0x7f1d2ea35ae8>  0.375221  \n",
       "1  <function zero_one_loss at 0x7f1d2ea35ae8>  0.403540  \n",
       "2  <function zero_one_loss at 0x7f1d2ea35ae8>  0.373451  \n",
       "3  <function zero_one_loss at 0x7f1d2ea35ae8>  0.364602  \n",
       "4  <function zero_one_loss at 0x7f1d2ea35ae8>  0.373451  \n",
       "5  <function zero_one_loss at 0x7f1d2ea35ae8>  0.364602  \n",
       "6  <function zero_one_loss at 0x7f1d2ea35ae8>  0.400000  \n",
       "7  <function zero_one_loss at 0x7f1d2ea35ae8>  0.385841  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xytv_df = mt.CVset_df(**mdset, ig=0)\n",
    "model_df = mt.cross_join(xytv_df, mdpar_df.apply(lambda x: mt.MDinit(**x.to_dict()), axis=1).to_frame(\"model\").join(mdpar_df[[\"namemd\", \"f_loss\"]]))\n",
    "model_df.apply(lambda x: mt.MDfit(**x.to_dict()), axis=1)\n",
    "model_df[\"loss\"] = model_df.apply(lambda x: mt.Loss(x[\"yv\"], mt.MDpred(**x.to_dict()), x[\"f_loss\"]), axis=1)\n",
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "      <th>std</th>\n",
       "      <th>Z-score</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>compound</th>\n",
       "      <td>0.035465</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>849.400636</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w__a(em&gt;</th>\n",
       "      <td>0.032842</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>784.134798</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>0.030734</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>731.697520</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_romney</th>\n",
       "      <td>0.030570</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>727.608793</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_sum</th>\n",
       "      <td>0.029642</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>704.536254</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neu</th>\n",
       "      <td>0.027016</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>639.198047</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>0.026045</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>615.020520</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w__a(a&gt;</th>\n",
       "      <td>0.024854</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>585.384351</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>0.023923</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>562.232707</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_sum</th>\n",
       "      <td>0.023251</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>545.516767</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>0.022421</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>524.861324</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_.</th>\n",
       "      <td>0.019021</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>440.265846</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w__a(url&gt;</th>\n",
       "      <td>0.017623</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>405.473582</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>0.015804</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>360.211145</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_mitt</th>\n",
       "      <td>0.011966</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>264.732670</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_:</th>\n",
       "      <td>0.010324</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>223.867904</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_,</th>\n",
       "      <td>0.010207</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>220.973511</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_obama</th>\n",
       "      <td>0.010069</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>217.531591</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_\"</th>\n",
       "      <td>0.009092</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>193.220334</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_?</th>\n",
       "      <td>0.008860</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>187.446907</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_!</th>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>183.478991</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_vote</th>\n",
       "      <td>0.008241</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>172.044996</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_win</th>\n",
       "      <td>0.008218</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>171.470058</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_debate</th>\n",
       "      <td>0.007164</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>145.249207</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_'</th>\n",
       "      <td>0.006278</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>123.205121</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_-</th>\n",
       "      <td>0.005865</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>112.934331</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_job</th>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>96.797997</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_poll</th>\n",
       "      <td>0.005215</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>96.747069</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_say</th>\n",
       "      <td>0.005184</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>95.989681</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_plan</th>\n",
       "      <td>0.005151</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>95.165612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_wa</th>\n",
       "      <td>0.005096</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>93.803163</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_dont</th>\n",
       "      <td>0.004853</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>87.742998</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_like</th>\n",
       "      <td>0.004785</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>86.065005</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_president</th>\n",
       "      <td>0.004738</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>84.885004</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_go</th>\n",
       "      <td>0.004582</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>81.017990</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_tax</th>\n",
       "      <td>0.004457</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>77.906315</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_#</th>\n",
       "      <td>0.004422</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>77.022311</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_get</th>\n",
       "      <td>0.004291</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>73.762656</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_woman</th>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>64.145143</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_lead</th>\n",
       "      <td>0.003597</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>56.501336</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_im</th>\n",
       "      <td>0.003584</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>56.182485</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_ha</th>\n",
       "      <td>0.003541</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>55.095778</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_think</th>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>51.435092</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_ryan</th>\n",
       "      <td>0.003389</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>51.318878</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_</th>\n",
       "      <td>0.003273</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>48.445724</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_via</th>\n",
       "      <td>0.003268</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>48.305481</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_would</th>\n",
       "      <td>0.003248</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>47.823196</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_know</th>\n",
       "      <td>0.003188</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>46.321069</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_/</th>\n",
       "      <td>0.003123</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>44.712681</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_u</th>\n",
       "      <td>0.003052</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>42.936250</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 freq       std     Z-score  p-value\n",
       "compound     0.035465  0.000204  849.400636      0.0\n",
       "w__a(em>     0.032842  0.000197  784.134798      0.0\n",
       "neg          0.030734  0.000191  731.697520      0.0\n",
       "w_romney     0.030570  0.000190  727.608793      0.0\n",
       "neg_sum      0.029642  0.000187  704.536254      0.0\n",
       "neu          0.027016  0.000179  639.198047      0.0\n",
       "word         0.026045  0.000176  615.020520      0.0\n",
       "w__a(a>      0.024854  0.000172  585.384351      0.0\n",
       "pos          0.023923  0.000169  562.232707      0.0\n",
       "pos_sum      0.023251  0.000166  545.516767      0.0\n",
       "time         0.022421  0.000163  524.861324      0.0\n",
       "w_.          0.019021  0.000151  440.265846      0.0\n",
       "w__a(url>    0.017623  0.000145  405.473582      0.0\n",
       "date         0.015804  0.000138  360.211145      0.0\n",
       "w_mitt       0.011966  0.000120  264.732670      0.0\n",
       "w_:          0.010324  0.000112  223.867904      0.0\n",
       "w_,          0.010207  0.000111  220.973511      0.0\n",
       "w_obama      0.010069  0.000110  217.531591      0.0\n",
       "w_\"          0.009092  0.000105  193.220334      0.0\n",
       "w_?          0.008860  0.000103  187.446907      0.0\n",
       "w_!          0.008701  0.000103  183.478991      0.0\n",
       "w_vote       0.008241  0.000100  172.044996      0.0\n",
       "w_win        0.008218  0.000100  171.470058      0.0\n",
       "w_debate     0.007164  0.000093  145.249207      0.0\n",
       "w_'          0.006278  0.000087  123.205121      0.0\n",
       "w_-          0.005865  0.000084  112.934331      0.0\n",
       "w_job        0.005217  0.000080   96.797997      0.0\n",
       "w_poll       0.005215  0.000080   96.747069      0.0\n",
       "w_say        0.005184  0.000079   95.989681      0.0\n",
       "w_plan       0.005151  0.000079   95.165612      0.0\n",
       "w_wa         0.005096  0.000079   93.803163      0.0\n",
       "w_dont       0.004853  0.000077   87.742998      0.0\n",
       "w_like       0.004785  0.000076   86.065005      0.0\n",
       "w_president  0.004738  0.000076   84.885004      0.0\n",
       "w_go         0.004582  0.000075   81.017990      0.0\n",
       "w_tax        0.004457  0.000074   77.906315      0.0\n",
       "w_#          0.004422  0.000073   77.022311      0.0\n",
       "w_get        0.004291  0.000072   73.762656      0.0\n",
       "w_woman      0.003904  0.000069   64.145143      0.0\n",
       "w_lead       0.003597  0.000066   56.501336      0.0\n",
       "w_im         0.003584  0.000066   56.182485      0.0\n",
       "w_ha         0.003541  0.000066   55.095778      0.0\n",
       "w_think      0.003394  0.000064   51.435092      0.0\n",
       "w_ryan       0.003389  0.000064   51.318878      0.0\n",
       "w_           0.003273  0.000063   48.445724      0.0\n",
       "w_via        0.003268  0.000063   48.305481      0.0\n",
       "w_would      0.003248  0.000063   47.823196      0.0\n",
       "w_know       0.003188  0.000062   46.321069      0.0\n",
       "w_/          0.003123  0.000062   44.712681      0.0\n",
       "w_u          0.003052  0.000061   42.936250      0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = mt.MDweight_analysis(model, xytv[\"xt\"])\n",
    "w.sort_values(\"freq\", ascending = False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.66      0.85      0.74       306\n",
      "        0.0       0.52      0.38      0.44       159\n",
      "        1.0       0.71      0.39      0.50       100\n",
      "\n",
      "avg / total       0.63      0.64      0.61       565\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predict</th>\n",
       "      <th>-1.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1.0</th>\n",
       "      <td>260</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>89</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>45</td>\n",
       "      <td>16</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predict  -1.0   0.0   1.0\n",
       "Class                    \n",
       "-1.0      260    40     6\n",
       " 0.0       89    60    10\n",
       " 1.0       45    16    39"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classification result and cross-table\n",
    "print(met.classification_report(xytv[\"yv\"].iloc[:,0], model.predict(xytv['xv'])))\n",
    "xytv[\"yv\"].assign(Predict=model.predict(xytv['xv'])).groupby([\"Class\", \"Predict\"])[\"Predict\"].count().unstack([\"Predict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class_-1.0</th>\n",
       "      <th>Class_0.0</th>\n",
       "      <th>Class_1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4970</th>\n",
       "      <td>-0.880</td>\n",
       "      <td>0.908</td>\n",
       "      <td>-0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>0.866</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>-0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>-0.864</td>\n",
       "      <td>0.946</td>\n",
       "      <td>-0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4942</th>\n",
       "      <td>-0.862</td>\n",
       "      <td>0.892</td>\n",
       "      <td>-0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2435</th>\n",
       "      <td>-0.858</td>\n",
       "      <td>0.886</td>\n",
       "      <td>-0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579</th>\n",
       "      <td>-0.856</td>\n",
       "      <td>0.880</td>\n",
       "      <td>-0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4010</th>\n",
       "      <td>0.848</td>\n",
       "      <td>-0.700</td>\n",
       "      <td>-0.148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.846</td>\n",
       "      <td>-0.802</td>\n",
       "      <td>-0.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3185</th>\n",
       "      <td>-0.846</td>\n",
       "      <td>0.886</td>\n",
       "      <td>-0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7044</th>\n",
       "      <td>0.818</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Class_-1.0  Class_0.0  Class_1.0\n",
       "4970      -0.880      0.908     -0.028\n",
       "3991       0.866     -0.546     -0.320\n",
       "2702      -0.864      0.946     -0.082\n",
       "4942      -0.862      0.892     -0.030\n",
       "2435      -0.858      0.886     -0.028\n",
       "5579      -0.856      0.880     -0.024\n",
       "4010       0.848     -0.700     -0.148\n",
       "229        0.846     -0.802     -0.044\n",
       "3185      -0.846      0.886     -0.040\n",
       "7044       0.818     -0.110     -0.708"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get most misclassified tweets\n",
    "ev = pd.get_dummies(xytv[\"yv\"].astype(\"O\")) - model.predict_proba(xytv['xv'])\n",
    "ie = ev.abs().sort_values([\"Class_-1.0\"], ascending=False).head(10).index\n",
    "ev.loc[ie]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<em>romney<em> lost #debate by #47percent',\n",
       " 'cu professors double-down on prediction of <em>romney<em> win due to economic factors http://t.co/zxk3ronu',\n",
       " \"@edshow wow they are idiots. i have been a ceo of nonprofits for 25 yrs & tell you they will <a>lose funding<a> over this. but <em>romney <em>don't care.\",\n",
       " '@maddow <em>romney<em> said alll ak47s are illegal .wrong #presidentialdebate',\n",
       " 'so <a>searching \"completely wrong\" on google images <a> yields pics of <em>romney<em> .. #mylifeismade',\n",
       " \"he helped in 'getting the olympics on track'?! #bitchplease all <em>romney<em> done was <a>criticize<a> #liar\",\n",
       " '<em>romney<em> rakes in staggering amount of money from lobbyists http://t.co/q7ejeurv',\n",
       " \"<em>romney<em>'s <a>tax plan doesn't add up<a>, but does it deserve a second look? http://t.co/ygfyftd7\",\n",
       " 'in 1965, mitt <em>romney<em> was arrested for <a>using large blocks of ice to slide down the slopes of a golf course<a>.',\n",
       " '#sadfact <em>romney<em>is going to win\"']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da.loc[ie, \"Anootated tweet\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>irts</th>\n",
       "      <th>namemd</th>\n",
       "      <th>f_loss</th>\n",
       "      <th>modelL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>Class\n",
       "0      -1.0\n",
       "2      -1.0\n",
       "3      -1....</td>\n",
       "      <td>[0, 3, 2, 2, 0, 1, 0, 1, 1, 6, 2, 5, 7, 9, 6, ...</td>\n",
       "      <td>ADB</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "      <td>[(DecisionTreeClassifier(class_weight=None, cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>Class\n",
       "0      -1.0\n",
       "2      -1.0\n",
       "3      -1....</td>\n",
       "      <td>[0, 3, 2, 2, 0, 1, 0, 1, 1, 6, 2, 5, 7, 9, 6, ...</td>\n",
       "      <td>BNB</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "      <td>[BernoulliNB(alpha=1.0, binarize=0.0, class_pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>Class\n",
       "0      -1.0\n",
       "2      -1.0\n",
       "3      -1....</td>\n",
       "      <td>[0, 3, 2, 2, 0, 1, 0, 1, 1, 6, 2, 5, 7, 9, 6, ...</td>\n",
       "      <td>GB</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "      <td>[([DecisionTreeRegressor(criterion='friedman_m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>Class\n",
       "0      -1.0\n",
       "2      -1.0\n",
       "3      -1....</td>\n",
       "      <td>[0, 3, 2, 2, 0, 1, 0, 1, 1, 6, 2, 5, 7, 9, 6, ...</td>\n",
       "      <td>LGB</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "      <td>[LGBMClassifier(boosting_type='gbdt', colsampl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>Class\n",
       "0      -1.0\n",
       "2      -1.0\n",
       "3      -1....</td>\n",
       "      <td>[0, 3, 2, 2, 0, 1, 0, 1, 1, 6, 2, 5, 7, 9, 6, ...</td>\n",
       "      <td>LGB DART</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "      <td>[LGBMClassifier(boosting_type='dart', colsampl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>Class\n",
       "0      -1.0\n",
       "2      -1.0\n",
       "3      -1....</td>\n",
       "      <td>[0, 3, 2, 2, 0, 1, 0, 1, 1, 6, 2, 5, 7, 9, 6, ...</td>\n",
       "      <td>RF</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "      <td>[(DecisionTreeClassifier(class_weight=None, cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>Class\n",
       "0      -1.0\n",
       "2      -1.0\n",
       "3      -1....</td>\n",
       "      <td>[0, 3, 2, 2, 0, 1, 0, 1, 1, 6, 2, 5, 7, 9, 6, ...</td>\n",
       "      <td>SVM</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "      <td>[LinearSVC(C=1.0, class_weight=None, dual=Fals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>date       time   pos_sum   neg_sum  com...</td>\n",
       "      <td>Class\n",
       "0      -1.0\n",
       "2      -1.0\n",
       "3      -1....</td>\n",
       "      <td>[0, 3, 2, 2, 0, 1, 0, 1, 1, 6, 2, 5, 7, 9, 6, ...</td>\n",
       "      <td>XGB</td>\n",
       "      <td>&lt;function zero_one_loss at 0x7f1d2ea35ae8&gt;</td>\n",
       "      <td>[XGBClassifier(base_score=0.5, colsample_bylev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   X  \\\n",
       "0        date       time   pos_sum   neg_sum  com...   \n",
       "1        date       time   pos_sum   neg_sum  com...   \n",
       "2        date       time   pos_sum   neg_sum  com...   \n",
       "3        date       time   pos_sum   neg_sum  com...   \n",
       "4        date       time   pos_sum   neg_sum  com...   \n",
       "5        date       time   pos_sum   neg_sum  com...   \n",
       "6        date       time   pos_sum   neg_sum  com...   \n",
       "7        date       time   pos_sum   neg_sum  com...   \n",
       "\n",
       "                                                   Y  \\\n",
       "0        Class\n",
       "0      -1.0\n",
       "2      -1.0\n",
       "3      -1....   \n",
       "1        Class\n",
       "0      -1.0\n",
       "2      -1.0\n",
       "3      -1....   \n",
       "2        Class\n",
       "0      -1.0\n",
       "2      -1.0\n",
       "3      -1....   \n",
       "3        Class\n",
       "0      -1.0\n",
       "2      -1.0\n",
       "3      -1....   \n",
       "4        Class\n",
       "0      -1.0\n",
       "2      -1.0\n",
       "3      -1....   \n",
       "5        Class\n",
       "0      -1.0\n",
       "2      -1.0\n",
       "3      -1....   \n",
       "6        Class\n",
       "0      -1.0\n",
       "2      -1.0\n",
       "3      -1....   \n",
       "7        Class\n",
       "0      -1.0\n",
       "2      -1.0\n",
       "3      -1....   \n",
       "\n",
       "                                                irts    namemd  \\\n",
       "0  [0, 3, 2, 2, 0, 1, 0, 1, 1, 6, 2, 5, 7, 9, 6, ...       ADB   \n",
       "1  [0, 3, 2, 2, 0, 1, 0, 1, 1, 6, 2, 5, 7, 9, 6, ...       BNB   \n",
       "2  [0, 3, 2, 2, 0, 1, 0, 1, 1, 6, 2, 5, 7, 9, 6, ...        GB   \n",
       "3  [0, 3, 2, 2, 0, 1, 0, 1, 1, 6, 2, 5, 7, 9, 6, ...       LGB   \n",
       "4  [0, 3, 2, 2, 0, 1, 0, 1, 1, 6, 2, 5, 7, 9, 6, ...  LGB DART   \n",
       "5  [0, 3, 2, 2, 0, 1, 0, 1, 1, 6, 2, 5, 7, 9, 6, ...        RF   \n",
       "6  [0, 3, 2, 2, 0, 1, 0, 1, 1, 6, 2, 5, 7, 9, 6, ...       SVM   \n",
       "7  [0, 3, 2, 2, 0, 1, 0, 1, 1, 6, 2, 5, 7, 9, 6, ...       XGB   \n",
       "\n",
       "                                       f_loss  \\\n",
       "0  <function zero_one_loss at 0x7f1d2ea35ae8>   \n",
       "1  <function zero_one_loss at 0x7f1d2ea35ae8>   \n",
       "2  <function zero_one_loss at 0x7f1d2ea35ae8>   \n",
       "3  <function zero_one_loss at 0x7f1d2ea35ae8>   \n",
       "4  <function zero_one_loss at 0x7f1d2ea35ae8>   \n",
       "5  <function zero_one_loss at 0x7f1d2ea35ae8>   \n",
       "6  <function zero_one_loss at 0x7f1d2ea35ae8>   \n",
       "7  <function zero_one_loss at 0x7f1d2ea35ae8>   \n",
       "\n",
       "                                              modelL  \n",
       "0  [(DecisionTreeClassifier(class_weight=None, cr...  \n",
       "1  [BernoulliNB(alpha=1.0, binarize=0.0, class_pr...  \n",
       "2  [([DecisionTreeRegressor(criterion='friedman_m...  \n",
       "3  [LGBMClassifier(boosting_type='gbdt', colsampl...  \n",
       "4  [LGBMClassifier(boosting_type='dart', colsampl...  \n",
       "5  [(DecisionTreeClassifier(class_weight=None, cr...  \n",
       "6  [LinearSVC(C=1.0, class_weight=None, dual=Fals...  \n",
       "7  [XGBClassifier(base_score=0.5, colsample_bylev...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdset_df = pd.DataFrame.from_records([mdset])\n",
    "md_df = mt.DMinit(mdset_df, mdpar_df)\n",
    "md_df.apply(lambda x: mt.CVply(f=mt.MDfit, parcv={\"model\": \"modelL\"}, **x.to_dict()), axis=1)\n",
    "md_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "md_df.to_pickle(\"CS583_Proj2_md_df\")\n",
    "md_df = pd.read_pickle(\"CS583_Proj2_md_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namemd\n",
       "ADB         0.397846\n",
       "BNB         0.403499\n",
       "GB          0.386512\n",
       "LGB         0.372166\n",
       "LGB DART    0.383499\n",
       "RF          0.386860\n",
       "SVM         0.391996\n",
       "XGB         0.383676\n",
       "Name: lossL, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_df[\"yvpL\"] = md_df.apply(lambda x: mt.CVply(f=mt.MDpred, parcv={\"model\": \"modelL\"}, **x.to_dict()), axis=1)\n",
    "md_df[\"lossL\"] = md_df.apply(lambda x: mt.CVply(f=mt.Loss, parcv={\"yp\": \"yvpL\"}, **x.to_dict()), axis=1)\n",
    "md_df.set_index([\"namemd\"])[\"lossL\"].apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37216713881019825"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yvp_combine = pd.concat([pd.concat(i) for i in md_df.loc[md_df[\"lossL\"].apply(np.mean).rank()==1, \"yvpL\"]], axis=1).apply(lambda x: st.mode(x)[0][0], axis=1)[mdset[\"Y\"].index]\n",
    "mt.Loss(mdset[\"Y\"].values, Yvp_combine.values, met.zero_one_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0       0.67      0.82      0.74      2893\n",
      "        0.0       0.54      0.44      0.48      1680\n",
      "        1.0       0.60      0.40      0.48      1075\n",
      "\n",
      "avg / total       0.62      0.63      0.61      5648\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Class</th>\n",
       "      <th>-1.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predict</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1.0</th>\n",
       "      <td>2380</td>\n",
       "      <td>780</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>388</td>\n",
       "      <td>737</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>125</td>\n",
       "      <td>163</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Class    -1.0   0.0   1.0\n",
       "Predict                  \n",
       "-1.0     2380   780   394\n",
       " 0.0      388   737   252\n",
       " 1.0      125   163   429"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classification result and cross-table\n",
    "print(met.classification_report(mdset[\"Y\"].loc[ir].values, Yvp_combine.values))\n",
    "mdset[\"Y\"].loc[ir].join(Yvp_combine.rename(\"Predict\")).groupby([\"Class\", \"Predict\"])[\"Class\"].count().unstack([\"Class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sequential Word Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7200, 36), (9969,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intcoder_seq = nt.Integer_Coder()\n",
    "seq = S.apply(lambda x: intcoder_seq.fit_transform(word_tokenize.transform(x, word_normalize.transform))).tolist()\n",
    "X_seq = pd.DataFrame(pad_sequences(seq, maxlen=int(np.percentile([len(i) for i in seq], 95))), S.index, dtype = \"int16\")\n",
    "s_word = pd.Series({**{val:key for key, val in intcoder_seq.code_dict.items()}, **{0:np.inf}})\n",
    "X_seq.shape, s_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/founderfan/anaconda3/lib/python3.6/site-packages/tables/leaf.py:394: PerformanceWarning: The Leaf ``/da/_i_table/index/sorted`` is exceeding the maximum recommended rowsize (104857600 bytes);\n",
      "be ready to see PyTables asking for *lots* of memory and possibly slow\n",
      "I/O.  You may want to reduce the rowsize by trimming the value of\n",
      "dimensions that are orthogonal (and preferably close) to the *main*\n",
      "dimension of this leave.  Alternatively, in case you have specified a\n",
      "very small/large chunksize, you may want to increase/decrease it.\n",
      "  PerformanceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6511, 200)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# daa = pd.read_csv(\"glove.twitter.27B.200d.zip\", delim_whitespace=True, header=None, index_col=0, dtype={0: \"str\"}, quotechar=None, quoting=3).sort_index().astype(\"float32\")\n",
    "# daa.iloc[:-1].to_hdf(\"glove.twitter.27B.200d.h5\", \"da\", format = \"t\", data_columns = [0], mode = \"w\", complevel = 5, complib = \"zlib\")\n",
    "store = pd.HDFStore(\"glove.twitter.27B.200d.h5\")\n",
    "ic_tmp = store.select_column(\"da\", 'index')\n",
    "daa = store.select('da', ic_tmp[ic_tmp.isin(s_word.values)].index)\n",
    "store.close()\n",
    "daa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Coverage: 0.6582509226879215\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>pos_swd</th>\n",
       "      <th>neg_swd</th>\n",
       "      <th>obj_swd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>inf</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.306466</td>\n",
       "      <td>-0.291534</td>\n",
       "      <td>0.384747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insidious</th>\n",
       "      <td>1.782293</td>\n",
       "      <td>-0.151287</td>\n",
       "      <td>-0.635173</td>\n",
       "      <td>0.086506</td>\n",
       "      <td>-0.666781</td>\n",
       "      <td>2.177102</td>\n",
       "      <td>-0.153495</td>\n",
       "      <td>2.319552</td>\n",
       "      <td>0.295783</td>\n",
       "      <td>-0.682121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.779379</td>\n",
       "      <td>-0.340853</td>\n",
       "      <td>0.218779</td>\n",
       "      <td>-1.769679</td>\n",
       "      <td>-1.371769</td>\n",
       "      <td>0.723667</td>\n",
       "      <td>0.730113</td>\n",
       "      <td>0.611867</td>\n",
       "      <td>4.280963</td>\n",
       "      <td>-3.270761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>1.483337</td>\n",
       "      <td>-0.482933</td>\n",
       "      <td>0.065889</td>\n",
       "      <td>-0.907354</td>\n",
       "      <td>-0.973925</td>\n",
       "      <td>0.748780</td>\n",
       "      <td>0.590777</td>\n",
       "      <td>0.699852</td>\n",
       "      <td>0.682770</td>\n",
       "      <td>0.734239</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.558152</td>\n",
       "      <td>-0.855162</td>\n",
       "      <td>0.669760</td>\n",
       "      <td>-3.429120</td>\n",
       "      <td>-1.111927</td>\n",
       "      <td>-0.128121</td>\n",
       "      <td>3.216916</td>\n",
       "      <td>-0.306466</td>\n",
       "      <td>-0.291534</td>\n",
       "      <td>0.384747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;em&gt;</th>\n",
       "      <td>0.243892</td>\n",
       "      <td>-0.159080</td>\n",
       "      <td>0.259676</td>\n",
       "      <td>1.283350</td>\n",
       "      <td>0.444095</td>\n",
       "      <td>-2.213586</td>\n",
       "      <td>-1.976314</td>\n",
       "      <td>1.079848</td>\n",
       "      <td>-2.385506</td>\n",
       "      <td>-0.884490</td>\n",
       "      <td>...</td>\n",
       "      <td>1.954455</td>\n",
       "      <td>-0.657083</td>\n",
       "      <td>0.548508</td>\n",
       "      <td>1.185473</td>\n",
       "      <td>1.808744</td>\n",
       "      <td>1.750447</td>\n",
       "      <td>-0.205635</td>\n",
       "      <td>-0.306466</td>\n",
       "      <td>-0.291534</td>\n",
       "      <td>0.384747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mitt</th>\n",
       "      <td>1.889940</td>\n",
       "      <td>0.134618</td>\n",
       "      <td>-1.266790</td>\n",
       "      <td>-0.935139</td>\n",
       "      <td>-0.814862</td>\n",
       "      <td>0.481039</td>\n",
       "      <td>-0.003477</td>\n",
       "      <td>-0.869186</td>\n",
       "      <td>0.785594</td>\n",
       "      <td>0.285991</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037611</td>\n",
       "      <td>-2.629066</td>\n",
       "      <td>0.260376</td>\n",
       "      <td>1.192170</td>\n",
       "      <td>-0.058297</td>\n",
       "      <td>-0.190996</td>\n",
       "      <td>0.025101</td>\n",
       "      <td>-0.306466</td>\n",
       "      <td>-0.291534</td>\n",
       "      <td>0.384747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 203 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1         2         3         4         5         6  \\\n",
       "inf        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "insidious  1.782293 -0.151287 -0.635173  0.086506 -0.666781  2.177102   \n",
       "!          1.483337 -0.482933  0.065889 -0.907354 -0.973925  0.748780   \n",
       "<em>       0.243892 -0.159080  0.259676  1.283350  0.444095 -2.213586   \n",
       "mitt       1.889940  0.134618 -1.266790 -0.935139 -0.814862  0.481039   \n",
       "\n",
       "                  7         8         9        10    ...          194  \\\n",
       "inf        0.000000  0.000000  0.000000  0.000000    ...     0.000000   \n",
       "insidious -0.153495  2.319552  0.295783 -0.682121    ...     0.779379   \n",
       "!          0.590777  0.699852  0.682770  0.734239    ...    -1.558152   \n",
       "<em>      -1.976314  1.079848 -2.385506 -0.884490    ...     1.954455   \n",
       "mitt      -0.003477 -0.869186  0.785594  0.285991    ...    -0.037611   \n",
       "\n",
       "                195       196       197       198       199       200  \\\n",
       "inf        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "insidious -0.340853  0.218779 -1.769679 -1.371769  0.723667  0.730113   \n",
       "!         -0.855162  0.669760 -3.429120 -1.111927 -0.128121  3.216916   \n",
       "<em>      -0.657083  0.548508  1.185473  1.808744  1.750447 -0.205635   \n",
       "mitt      -2.629066  0.260376  1.192170 -0.058297 -0.190996  0.025101   \n",
       "\n",
       "            pos_swd   neg_swd   obj_swd  \n",
       "inf       -0.306466 -0.291534  0.384747  \n",
       "insidious  0.611867  4.280963 -3.270761  \n",
       "!         -0.306466 -0.291534  0.384747  \n",
       "<em>      -0.306466 -0.291534  0.384747  \n",
       "mitt      -0.306466 -0.291534  0.384747  \n",
       "\n",
       "[5 rows x 203 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_embed = daa.reindex(s_word).join(daa_sent_c).fillna({\"pos_swd\": 0, \"neg_swd\": 0, \"obj_swd\": 1})\n",
    "print(\"Dictionary Coverage: {}\".format(da_embed.notnull().mean().mean()))\n",
    "da_embed = mt.CLscale(da_embed)\n",
    "da_embed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7200, 36), (24,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intcoder_tag = nt.Integer_Coder()\n",
    "seq_tag = [intcoder_tag.fit_transform([re.sub(r'[^\\w]', '', j[1][:2]) for j in i]) for i in nltk.pos_tag_sents(S.apply(word_tokenize.transform).tolist())]\n",
    "X_seq_tag = pd.DataFrame(pad_sequences(seq_tag, maxlen=int(np.percentile([len(i) for i in seq_tag], 95))), S.index, dtype = \"int16\")\n",
    "s_tag = pd.Series({**{val:key for key, val in intcoder_tag.code_dict.items()}, **{0:0}})\n",
    "X_seq_tag.shape, s_tag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>-0.208514</td>\n",
       "      <td>4.587317</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC</th>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>4.587317</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD</th>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DT</th>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>4.587317</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "      <td>-0.208514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6         7   \\\n",
       "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "   -0.208514  4.587317 -0.208514 -0.208514 -0.208514 -0.208514 -0.208514   \n",
       "CC -0.208514 -0.208514 -0.208514 -0.208514 -0.208514 -0.208514 -0.208514   \n",
       "CD -0.208514 -0.208514 -0.208514 -0.208514 -0.208514 -0.208514 -0.208514   \n",
       "DT -0.208514 -0.208514 -0.208514 -0.208514 -0.208514 -0.208514 -0.208514   \n",
       "\n",
       "          8         9         10    ...           14        15        16  \\\n",
       "0   0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "   -0.208514 -0.208514 -0.208514    ...    -0.208514 -0.208514 -0.208514   \n",
       "CC  4.587317 -0.208514 -0.208514    ...    -0.208514 -0.208514 -0.208514   \n",
       "CD -0.208514 -0.208514 -0.208514    ...    -0.208514 -0.208514 -0.208514   \n",
       "DT -0.208514 -0.208514  4.587317    ...    -0.208514 -0.208514 -0.208514   \n",
       "\n",
       "          17        18        19        20        21        22        23  \n",
       "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "   -0.208514 -0.208514 -0.208514 -0.208514 -0.208514 -0.208514 -0.208514  \n",
       "CC -0.208514 -0.208514 -0.208514 -0.208514 -0.208514 -0.208514 -0.208514  \n",
       "CD -0.208514 -0.208514 -0.208514 -0.208514 -0.208514 -0.208514 -0.208514  \n",
       "DT -0.208514 -0.208514 -0.208514 -0.208514 -0.208514 -0.208514 -0.208514  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_tag_embed = mt.CLscale(pd.get_dummies(s_tag).drop([0])).T\n",
    "da_tag_embed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdset_seq = mt.CVdata(df = X_seq.join(da[\"Class\"].astype(\"O\")), ic_x = X_seq.columns, ic_y = [\"Class\"], ir=ir, k=10, f_norm_y=pd.get_dummies, f_norm=None)\n",
    "mdset_tag = mt.CVdata(df = X_seq_tag.join(da[\"Class\"].astype(\"O\")), ic_x = X_seq_tag.columns, ic_y = [\"Class\"], ir=ir, k=10, f_norm_y=pd.get_dummies, f_norm=None)\n",
    "mdset = mt.CVdata(df = X.join(da[\"Class\"].astype(\"O\")), ic_x = list(X_sent.columns)+[\"date\", \"time\"], ic_y = [\"Class\"], ir=ir, k=10, f_norm_y=pd.get_dummies, f_norm=mt.CLscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xytv = mt.CVset(**mdset, ig=2)\n",
    "xytv_seq = mt.CVset(**mdset_seq, ig=2)\n",
    "xytv_tag = mt.CVset(**mdset_tag, ig=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_seq = kr.layers.Input(shape=(X_seq.shape[1],), dtype='int32')\n",
    "x = kr.layers.Embedding(*da_embed.shape, weights=[da_embed.values], trainable=False)(input_seq)\n",
    "input_seq_tag = kr.layers.Input(shape=(X_seq_tag.shape[1],), dtype='int32')\n",
    "embed_tag = kr.layers.Embedding(*da_tag_embed.shape, weights=[da_tag_embed.values], trainable=False)(input_seq_tag)\n",
    "x = kr.layers.concatenate([x, embed_tag], axis=2)\n",
    "x = kr.layers.Dropout(0.5)(x)\n",
    "x = kr.layers.Convolution1D(256, 1, activation = 'relu')(x)\n",
    "x_br1 = kr.layers.Dropout(0.5)(x)\n",
    "x_br1 = kr.layers.MaxPooling1D(2)(x_br1)\n",
    "x_br1 = kr.layers.Convolution1D(16, 2, activation = 'relu')(x_br1)\n",
    "x_br1 = kr.layers.MaxPooling1D(3)(x_br1)\n",
    "x_br1 = kr.layers.Flatten()(x_br1)\n",
    "x = kr.layers.Dropout(0.5)(x)\n",
    "x = kr.layers.LSTM(48, activation = 'relu')(x)\n",
    "input_base = kr.layers.Input(shape=(mdset[\"X\"].shape[1],))\n",
    "x = kr.layers.concatenate([x, x_br1, input_base], axis=1)\n",
    "x = kr.layers.Dropout(0.5)(x)\n",
    "x = kr.layers.Dense(256, activation = 'relu')(x)\n",
    "x = kr.layers.Dropout(0.5)(x)\n",
    "preds = kr.layers.Dense(mdset[\"Y\"].shape[1], activation='softmax')(x)\n",
    "model = kr.models.Model([input_seq, input_seq_tag, input_base], preds)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=kr.optimizers.Nadam(5e-4), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 36)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 36)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 36, 203)       2023707                                      \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 36, 23)        552                                          \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 36, 226)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 36, 226)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 36, 256)       58112                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 36, 256)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)   (None, 18, 256)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 17, 16)        8208                                         \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 36, 256)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)   (None, 5, 16)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 48)            58560                                        \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 80)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, 8)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 136)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 136)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 256)           35072                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 256)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 3)             771                                          \n",
      "====================================================================================================\n",
      "Total params: 2,184,982.0\n",
      "Trainable params: 160,723.0\n",
      "Non-trainable params: 2,024,259.0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Obama\n",
    "# LSTM 64 + CNN 12*5 cnn2-max3\n",
    "[0.6720, 0.6400, 0.6444, 0.6196, np.nan]\n",
    "# LSTM 48 + CNN 16*5 cnn2-max3\n",
    "[0.6658, 0.6622, 0.6471, 0.6400, 0.6569]\n",
    "# LSTM 48 + CNN 16*5 cnn3-max3\n",
    "[0.6676, 0.6560, np.nan, 0.6213, 0.6480]\n",
    "# GRU 48 + CNN 16*5 cnn2-max3\n",
    "[np.nan, np.nan, np.nan, 0.6418, np.nan]\n",
    "# GRU 48 + CNN 16*6 cnn3-pad1-max3\n",
    "[np.nan, np.nan, np.nan, np.nan, 0.6533]\n",
    "# GRU 48 + CNN 16*4 cnn3-max4\n",
    "[np.nan, 0.6551, np.nan, np.nan, np.nan]\n",
    "# LSTM 32 + CNN 20*5 cnn2-max3\n",
    "[0.6596, np.nan, 0.6480, np.nan, 0.6604]\n",
    "# LSTM 24 + CNN 24*5 cnn2-max3\n",
    "[np.nan, 0.6400, np.nan, 0.6311, np.nan]\n",
    "# LSTM 12 + CNN 28*5 cnn2-max3\n",
    "[0.6409, np.nan, np.nan, np.nan, 0.6400]\n",
    "\n",
    "## Romney\n",
    "# LSTM 48 + CNN 16*5 cnn2-max3\n",
    "[0.6354, 0.6398, 0.6469, np.nan, np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4518 samples, validate on 1130 samples\n",
      "Epoch 1/50\n",
      "19s - loss: 1.0038 - acc: 0.5062 - val_loss: 0.9411 - val_acc: 0.5690\n",
      "Epoch 2/50\n",
      "15s - loss: 0.9575 - acc: 0.5458 - val_loss: 0.9254 - val_acc: 0.5788\n",
      "Epoch 3/50\n",
      "15s - loss: 0.9446 - acc: 0.5507 - val_loss: 0.8914 - val_acc: 0.6088\n",
      "Epoch 4/50\n",
      "16s - loss: 0.9289 - acc: 0.5602 - val_loss: 0.8847 - val_acc: 0.6159\n",
      "Epoch 5/50\n",
      "17s - loss: 0.9229 - acc: 0.5637 - val_loss: 0.8732 - val_acc: 0.6283\n",
      "Epoch 6/50\n",
      "20s - loss: 0.9095 - acc: 0.5810 - val_loss: 0.8726 - val_acc: 0.6150\n",
      "Epoch 7/50\n",
      "18s - loss: 0.8945 - acc: 0.5839 - val_loss: 0.8868 - val_acc: 0.6000\n",
      "Epoch 8/50\n",
      "22s - loss: 0.9073 - acc: 0.5799 - val_loss: 0.8536 - val_acc: 0.6283\n",
      "Epoch 9/50\n",
      "22s - loss: 0.8838 - acc: 0.5965 - val_loss: 0.8555 - val_acc: 0.6283\n",
      "Epoch 10/50\n",
      "20s - loss: 0.8714 - acc: 0.5994 - val_loss: 0.8548 - val_acc: 0.6186\n",
      "Epoch 11/50\n",
      "16s - loss: 0.8720 - acc: 0.6036 - val_loss: 0.8374 - val_acc: 0.6425\n",
      "Epoch 12/50\n",
      "21s - loss: 0.8674 - acc: 0.6014 - val_loss: 0.8765 - val_acc: 0.5956\n",
      "Epoch 13/50\n",
      "21s - loss: 0.8533 - acc: 0.6124 - val_loss: 0.8504 - val_acc: 0.6230\n",
      "Epoch 14/50\n",
      "25s - loss: 0.8546 - acc: 0.6118 - val_loss: 0.8371 - val_acc: 0.6372\n",
      "Epoch 15/50\n",
      "25s - loss: 0.8370 - acc: 0.6246 - val_loss: 0.8411 - val_acc: 0.6195\n",
      "Epoch 16/50\n",
      "39s - loss: 0.8449 - acc: 0.6186 - val_loss: 0.8607 - val_acc: 0.6195\n",
      "Epoch 17/50\n",
      "26s - loss: 0.8351 - acc: 0.6248 - val_loss: 0.8361 - val_acc: 0.6434\n",
      "Epoch 18/50\n",
      "24s - loss: 0.8291 - acc: 0.6262 - val_loss: 0.8258 - val_acc: 0.6398\n",
      "Epoch 19/50\n",
      "21s - loss: 0.8222 - acc: 0.6352 - val_loss: 0.8260 - val_acc: 0.6451\n",
      "Epoch 20/50\n",
      "24s - loss: 0.8136 - acc: 0.6304 - val_loss: 0.8247 - val_acc: 0.6460\n",
      "Epoch 21/50\n",
      "21s - loss: 0.8171 - acc: 0.6332 - val_loss: 0.8242 - val_acc: 0.6381\n",
      "Epoch 22/50\n",
      "23s - loss: 0.7956 - acc: 0.6419 - val_loss: 0.8199 - val_acc: 0.6522\n",
      "Epoch 23/50\n",
      "25s - loss: 0.8176 - acc: 0.6341 - val_loss: 0.8302 - val_acc: 0.6363\n",
      "Epoch 24/50\n",
      "23s - loss: 0.8056 - acc: 0.6423 - val_loss: 0.8307 - val_acc: 0.6389\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 00024: reducing learning rate to 0.0002500000118743628.\n",
      "32s - loss: 0.8084 - acc: 0.6456 - val_loss: 0.8413 - val_acc: 0.6283\n",
      "Epoch 26/50\n",
      "23s - loss: 0.7935 - acc: 0.6467 - val_loss: 0.8207 - val_acc: 0.6496\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 00026: reducing learning rate to 0.0001250000059371814.\n",
      "29s - loss: 0.7800 - acc: 0.6574 - val_loss: 0.8219 - val_acc: 0.6434\n",
      "Epoch 28/50\n",
      "23s - loss: 0.7648 - acc: 0.6510 - val_loss: 0.8173 - val_acc: 0.6354\n",
      "Epoch 29/50\n",
      "27s - loss: 0.7669 - acc: 0.6583 - val_loss: 0.8127 - val_acc: 0.6434\n",
      "Epoch 30/50\n",
      "20s - loss: 0.7730 - acc: 0.6576 - val_loss: 0.8181 - val_acc: 0.6407\n",
      "Epoch 31/50\n",
      "28s - loss: 0.7697 - acc: 0.6565 - val_loss: 0.8128 - val_acc: 0.6363\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 00031: reducing learning rate to 6.25000029685907e-05.\n",
      "27s - loss: 0.7656 - acc: 0.6618 - val_loss: 0.8187 - val_acc: 0.6363\n",
      "Epoch 33/50\n",
      "31s - loss: 0.7654 - acc: 0.6594 - val_loss: 0.8109 - val_acc: 0.6407\n",
      "Epoch 34/50\n",
      "22s - loss: 0.7565 - acc: 0.6645 - val_loss: 0.8157 - val_acc: 0.6381\n",
      "Epoch 35/50\n",
      "26s - loss: 0.7691 - acc: 0.6600 - val_loss: 0.8121 - val_acc: 0.6442\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 00035: reducing learning rate to 3.125000148429535e-05.\n",
      "25s - loss: 0.7623 - acc: 0.6547 - val_loss: 0.8121 - val_acc: 0.6425\n",
      "Epoch 37/50\n",
      "22s - loss: 0.7491 - acc: 0.6709 - val_loss: 0.8136 - val_acc: 0.6407\n",
      "Epoch 38/50\n",
      "19s - loss: 0.7459 - acc: 0.6751 - val_loss: 0.8099 - val_acc: 0.6425\n",
      "Epoch 39/50\n",
      "17s - loss: 0.7745 - acc: 0.6549 - val_loss: 0.8145 - val_acc: 0.6416\n",
      "Epoch 40/50\n",
      "19s - loss: 0.7565 - acc: 0.6589 - val_loss: 0.8135 - val_acc: 0.6425\n",
      "Epoch 41/50\n",
      "\n",
      "Epoch 00040: reducing learning rate to 1.5625000742147677e-05.\n",
      "18s - loss: 0.7645 - acc: 0.6602 - val_loss: 0.8110 - val_acc: 0.6416\n",
      "Epoch 42/50\n",
      "17s - loss: 0.7529 - acc: 0.6669 - val_loss: 0.8123 - val_acc: 0.6442\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 00042: reducing learning rate to 7.812500371073838e-06.\n",
      "17s - loss: 0.7454 - acc: 0.6649 - val_loss: 0.8139 - val_acc: 0.6460\n",
      "Epoch 44/50\n",
      "22s - loss: 0.7509 - acc: 0.6707 - val_loss: 0.8136 - val_acc: 0.6460\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 00044: reducing learning rate to 3.906250185536919e-06.\n",
      "25s - loss: 0.7513 - acc: 0.6684 - val_loss: 0.8132 - val_acc: 0.6469\n",
      "Epoch 46/50\n",
      "19s - loss: 0.7453 - acc: 0.6768 - val_loss: 0.8128 - val_acc: 0.6469\n",
      "Epoch 47/50\n",
      "\n",
      "Epoch 00046: reducing learning rate to 1.9531250927684596e-06.\n",
      "23s - loss: 0.7544 - acc: 0.6680 - val_loss: 0.8128 - val_acc: 0.6469\n",
      "Epoch 48/50\n",
      "23s - loss: 0.7627 - acc: 0.6713 - val_loss: 0.8128 - val_acc: 0.6469\n",
      "Epoch 49/50\n",
      "\n",
      "Epoch 00048: reducing learning rate to 1e-06.\n",
      "27s - loss: 0.7623 - acc: 0.6622 - val_loss: 0.8128 - val_acc: 0.6469\n",
      "Epoch 50/50\n",
      "21s - loss: 0.7534 - acc: 0.6591 - val_loss: 0.8128 - val_acc: 0.6469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe02c5f4e10>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([xytv_seq[\"xt\"].values, xytv_tag[\"xt\"].values, xytv[\"xt\"].values], xytv[\"yt\"].values, \n",
    "          validation_data=([xytv_seq[\"xv\"].values, xytv_tag[\"xv\"].values, xytv[\"xv\"].values], xytv[\"yv\"].values), \n",
    "          callbacks=[kr.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=1)], \n",
    "          epochs=50, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 1125 samples\n",
      "Epoch 1/50\n",
      "15s - loss: 1.0629 - acc: 0.4373 - val_loss: 0.9691 - val_acc: 0.5440\n",
      "Epoch 2/50\n",
      "16s - loss: 1.0013 - acc: 0.5067 - val_loss: 0.9299 - val_acc: 0.5769\n",
      "Epoch 3/50\n",
      "15s - loss: 1.3854 - acc: 0.4644 - val_loss: 0.9415 - val_acc: 0.5662\n",
      "Epoch 4/50\n",
      "13s - loss: 1.0038 - acc: 0.5164 - val_loss: 0.9364 - val_acc: 0.5600\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 00004: reducing learning rate to 0.0002500000118743628.\n",
      "14s - loss: 0.9849 - acc: 0.5229 - val_loss: 0.9360 - val_acc: 0.5698\n",
      "Epoch 6/50\n",
      "13s - loss: 0.9720 - acc: 0.5324 - val_loss: 0.9332 - val_acc: 0.5662\n",
      "Epoch 7/50\n",
      "14s - loss: 0.9656 - acc: 0.5349 - val_loss: 0.9262 - val_acc: 0.5760\n",
      "Epoch 8/50\n",
      "14s - loss: 0.9618 - acc: 0.5413 - val_loss: 0.9210 - val_acc: 0.5867\n",
      "Epoch 9/50\n",
      "14s - loss: 0.9640 - acc: 0.5400 - val_loss: 0.9261 - val_acc: 0.5831\n",
      "Epoch 10/50\n",
      "14s - loss: 0.9653 - acc: 0.5424 - val_loss: 0.9218 - val_acc: 0.5920\n",
      "Epoch 11/50\n",
      "14s - loss: 0.9579 - acc: 0.5436 - val_loss: 0.9197 - val_acc: 0.5858\n",
      "Epoch 12/50\n",
      "14s - loss: 0.9601 - acc: 0.5471 - val_loss: 0.9180 - val_acc: 0.5956\n",
      "Epoch 13/50\n",
      "13s - loss: 0.9467 - acc: 0.5509 - val_loss: 0.9170 - val_acc: 0.6000\n",
      "Epoch 14/50\n",
      "13s - loss: 0.9470 - acc: 0.5471 - val_loss: 0.9150 - val_acc: 0.5707\n",
      "Epoch 15/50\n",
      "13s - loss: 0.9436 - acc: 0.5542 - val_loss: 0.9056 - val_acc: 0.6098\n",
      "Epoch 16/50\n",
      "13s - loss: 0.9429 - acc: 0.5493 - val_loss: 0.9063 - val_acc: 0.6027\n",
      "Epoch 17/50\n",
      "13s - loss: 0.9437 - acc: 0.5562 - val_loss: 0.9039 - val_acc: 0.6044\n",
      "Epoch 18/50\n",
      "18s - loss: 0.9381 - acc: 0.5516 - val_loss: 0.9054 - val_acc: 0.6018\n",
      "Epoch 19/50\n",
      "15s - loss: 0.9233 - acc: 0.5711 - val_loss: 0.8955 - val_acc: 0.6044\n",
      "Epoch 20/50\n",
      "14s - loss: 0.9388 - acc: 0.5627 - val_loss: 0.8954 - val_acc: 0.6169\n",
      "Epoch 21/50\n",
      "14s - loss: 0.9209 - acc: 0.5691 - val_loss: 0.8902 - val_acc: 0.6160\n",
      "Epoch 22/50\n",
      "14s - loss: 0.9224 - acc: 0.5620 - val_loss: 0.8935 - val_acc: 0.6071\n",
      "Epoch 23/50\n",
      "19s - loss: 0.9182 - acc: 0.5678 - val_loss: 0.8882 - val_acc: 0.6142\n",
      "Epoch 24/50\n",
      "15s - loss: 0.9028 - acc: 0.5800 - val_loss: 0.8823 - val_acc: 0.6169\n",
      "Epoch 25/50\n",
      "15s - loss: 0.9115 - acc: 0.5731 - val_loss: 0.8788 - val_acc: 0.6142\n",
      "Epoch 26/50\n",
      "15s - loss: 0.9049 - acc: 0.5818 - val_loss: 0.8752 - val_acc: 0.6089\n",
      "Epoch 27/50\n",
      "15s - loss: 0.9140 - acc: 0.5760 - val_loss: 0.8730 - val_acc: 0.6213\n",
      "Epoch 28/50\n",
      "15s - loss: 0.9015 - acc: 0.5884 - val_loss: 0.8693 - val_acc: 0.6116\n",
      "Epoch 29/50\n",
      "16s - loss: 0.9039 - acc: 0.5769 - val_loss: 0.8696 - val_acc: 0.6231\n",
      "Epoch 30/50\n",
      "16s - loss: 0.8952 - acc: 0.5847 - val_loss: 0.8700 - val_acc: 0.6222\n",
      "Epoch 31/50\n",
      "17s - loss: 0.8932 - acc: 0.5936 - val_loss: 0.8653 - val_acc: 0.6302\n",
      "Epoch 32/50\n",
      "15s - loss: 0.8938 - acc: 0.5920 - val_loss: 0.8618 - val_acc: 0.6258\n",
      "Epoch 33/50\n",
      "15s - loss: 0.8833 - acc: 0.6062 - val_loss: 0.8644 - val_acc: 0.6320\n",
      "Epoch 34/50\n",
      "17s - loss: 0.8913 - acc: 0.5873 - val_loss: 0.8554 - val_acc: 0.6249\n",
      "Epoch 35/50\n",
      "14s - loss: 0.8763 - acc: 0.6031 - val_loss: 0.8569 - val_acc: 0.6249\n",
      "Epoch 36/50\n",
      "13s - loss: 0.8854 - acc: 0.5891 - val_loss: 0.8507 - val_acc: 0.6329\n",
      "Epoch 37/50\n",
      "14s - loss: 0.8859 - acc: 0.5940 - val_loss: 0.8561 - val_acc: 0.6382\n",
      "Epoch 38/50\n",
      "14s - loss: 0.8679 - acc: 0.6078 - val_loss: 0.8501 - val_acc: 0.6356\n",
      "Epoch 39/50\n",
      "14s - loss: 0.8800 - acc: 0.5913 - val_loss: 0.8446 - val_acc: 0.6364\n",
      "Epoch 40/50\n",
      "13s - loss: 0.8612 - acc: 0.6073 - val_loss: 0.8494 - val_acc: 0.6382\n",
      "Epoch 41/50\n",
      "13s - loss: 0.8547 - acc: 0.6140 - val_loss: 0.8399 - val_acc: 0.6382\n",
      "Epoch 42/50\n",
      "14s - loss: 0.8526 - acc: 0.6191 - val_loss: 0.8359 - val_acc: 0.6400\n",
      "Epoch 43/50\n",
      "15s - loss: 0.8652 - acc: 0.6104 - val_loss: 0.8494 - val_acc: 0.6418\n",
      "Epoch 44/50\n",
      "14s - loss: 0.8587 - acc: 0.6242 - val_loss: 0.8381 - val_acc: 0.6400\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 00044: reducing learning rate to 0.0001250000059371814.\n",
      "14s - loss: 0.8524 - acc: 0.6136 - val_loss: 0.8359 - val_acc: 0.6462\n",
      "Epoch 46/50\n",
      "14s - loss: 0.8424 - acc: 0.6104 - val_loss: 0.8352 - val_acc: 0.6400\n",
      "Epoch 47/50\n",
      "15s - loss: 0.8379 - acc: 0.6247 - val_loss: 0.8349 - val_acc: 0.6409\n",
      "Epoch 48/50\n",
      "14s - loss: 0.8440 - acc: 0.6160 - val_loss: 0.8327 - val_acc: 0.6418\n",
      "Epoch 49/50\n",
      "14s - loss: 0.8305 - acc: 0.6211 - val_loss: 0.8273 - val_acc: 0.6427\n",
      "Epoch 50/50\n",
      "14s - loss: 0.8458 - acc: 0.6204 - val_loss: 0.8338 - val_acc: 0.6400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb63adcac8>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([xytv_seq[\"xt\"].values, xytv_tag[\"xt\"].values, xytv[\"xt\"].values], xytv[\"yt\"].values, \n",
    "          validation_data=([xytv_seq[\"xv\"].values, xytv_tag[\"xv\"].values, xytv[\"xv\"].values], xytv[\"yv\"].values), \n",
    "          callbacks=[kr.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=1)], \n",
    "          epochs=50, batch_size=32, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}